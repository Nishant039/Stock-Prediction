{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bSTPGJ_pmQcF",
    "outputId": "b39d33ff-4c63-487b-cc72-108d05034cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZTYtVJxmYEk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JYwwG9WtmiwL",
    "outputId": "23c7c41b-9cdd-487d-8309-780bd38063b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1375085, 49)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/content/drive/My Drive/stock_model_data_v3.0.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bzwhP9SJmkzv",
    "outputId": "ad67bf5a-7c2d-40c0-c403-2b1c59344d20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323375, 49)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"]=pd.to_datetime(df[\"Date\"])\n",
    "df_2019_2020=df[(df[\"Date\"] >= \"2019-01-01\") ]\n",
    "df_2019_2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9H8zX_Yims2_",
    "outputId": "c2b6e6ff-f77c-4f95-8b1f-a4f5050e177e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281199, 49)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_2019_2020.iloc[0:1,:]\n",
    "def prepare_train(df):\n",
    "  \n",
    "    global df_train\n",
    "    df_subset=df[df[\"Date\"]<'2020-01-01']\n",
    "    df_train=df_train.append(df_subset)\n",
    "        \n",
    "df_2019_2020.iloc[1:,:].groupby('Symbols').apply(prepare_train) \n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "bBnqTZoUmxFd",
    "outputId": "dea06db3-75f8-41c9-c5be-991760e68f75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbols</th>\n",
       "      <th>Date</th>\n",
       "      <th>bin_gain</th>\n",
       "      <th>bin_gain_1%</th>\n",
       "      <th>bin_gain_2%</th>\n",
       "      <th>SMA_PCH5</th>\n",
       "      <th>SMA_PCH15</th>\n",
       "      <th>SMA_PCH50</th>\n",
       "      <th>EMA_PCH5</th>\n",
       "      <th>EMA_PCH15</th>\n",
       "      <th>EMA_PCH50</th>\n",
       "      <th>ROC5</th>\n",
       "      <th>ROC15</th>\n",
       "      <th>ROC50</th>\n",
       "      <th>MOM_PCH5</th>\n",
       "      <th>MOM_PCH15</th>\n",
       "      <th>MOM_PCH50</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>RSI</th>\n",
       "      <th>StochK</th>\n",
       "      <th>StochD</th>\n",
       "      <th>CMO</th>\n",
       "      <th>AroonUp</th>\n",
       "      <th>AroonDown</th>\n",
       "      <th>ChaikinAD</th>\n",
       "      <th>MFI</th>\n",
       "      <th>WillR</th>\n",
       "      <th>ULTOS</th>\n",
       "      <th>MA_BIN5</th>\n",
       "      <th>MA_BIN15</th>\n",
       "      <th>MA_BIN50</th>\n",
       "      <th>EMA_BIN5</th>\n",
       "      <th>EMA_BIN15</th>\n",
       "      <th>EMA_BIN50</th>\n",
       "      <th>ROC_BIN5</th>\n",
       "      <th>ROC_BIN15</th>\n",
       "      <th>ROC_BIN50</th>\n",
       "      <th>MOM_BIN5</th>\n",
       "      <th>MOM_BIN15</th>\n",
       "      <th>MOM_BIN50</th>\n",
       "      <th>macd_BIN</th>\n",
       "      <th>Beta_DOW</th>\n",
       "      <th>Beta_NDQ</th>\n",
       "      <th>Beta_NKK</th>\n",
       "      <th>Beta_NFTY</th>\n",
       "      <th>Beta_EURINR</th>\n",
       "      <th>Beta_GBPINR</th>\n",
       "      <th>Beta_USDINR</th>\n",
       "      <th>Beta_JPYINR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272888</td>\n",
       "      <td>0.153890</td>\n",
       "      <td>0.118366</td>\n",
       "      <td>0.382373</td>\n",
       "      <td>0.204310</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.100874</td>\n",
       "      <td>0.093447</td>\n",
       "      <td>0.092397</td>\n",
       "      <td>0.924910</td>\n",
       "      <td>0.960334</td>\n",
       "      <td>0.964154</td>\n",
       "      <td>0.645913</td>\n",
       "      <td>0.578339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.578339</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.754551</td>\n",
       "      <td>0.559276</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.410535</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659376</td>\n",
       "      <td>0.621633</td>\n",
       "      <td>0.613479</td>\n",
       "      <td>0.440109</td>\n",
       "      <td>0.374381</td>\n",
       "      <td>0.506552</td>\n",
       "      <td>0.615799</td>\n",
       "      <td>0.372038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268255</td>\n",
       "      <td>0.152835</td>\n",
       "      <td>0.117679</td>\n",
       "      <td>0.376302</td>\n",
       "      <td>0.202387</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>0.099852</td>\n",
       "      <td>0.090290</td>\n",
       "      <td>0.094353</td>\n",
       "      <td>0.924120</td>\n",
       "      <td>0.958868</td>\n",
       "      <td>0.964945</td>\n",
       "      <td>0.646127</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.744074</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.754507</td>\n",
       "      <td>0.678977</td>\n",
       "      <td>0.582417</td>\n",
       "      <td>0.382327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659186</td>\n",
       "      <td>0.622859</td>\n",
       "      <td>0.612647</td>\n",
       "      <td>0.438002</td>\n",
       "      <td>0.372540</td>\n",
       "      <td>0.506837</td>\n",
       "      <td>0.616610</td>\n",
       "      <td>0.376306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.150536</td>\n",
       "      <td>0.115880</td>\n",
       "      <td>0.369681</td>\n",
       "      <td>0.198916</td>\n",
       "      <td>0.129764</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.087824</td>\n",
       "      <td>0.093363</td>\n",
       "      <td>0.922739</td>\n",
       "      <td>0.957652</td>\n",
       "      <td>0.964549</td>\n",
       "      <td>0.646143</td>\n",
       "      <td>0.518346</td>\n",
       "      <td>0.698114</td>\n",
       "      <td>0.874214</td>\n",
       "      <td>0.518346</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.639004</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.413571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665346</td>\n",
       "      <td>0.628724</td>\n",
       "      <td>0.615041</td>\n",
       "      <td>0.449382</td>\n",
       "      <td>0.380856</td>\n",
       "      <td>0.513757</td>\n",
       "      <td>0.612551</td>\n",
       "      <td>0.360319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253621</td>\n",
       "      <td>0.146179</td>\n",
       "      <td>0.112324</td>\n",
       "      <td>0.361238</td>\n",
       "      <td>0.193167</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.085590</td>\n",
       "      <td>0.095790</td>\n",
       "      <td>0.919647</td>\n",
       "      <td>0.956493</td>\n",
       "      <td>0.965505</td>\n",
       "      <td>0.645930</td>\n",
       "      <td>0.441681</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.441681</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.754478</td>\n",
       "      <td>0.617409</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.381936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.643412</td>\n",
       "      <td>0.603874</td>\n",
       "      <td>0.597839</td>\n",
       "      <td>0.439958</td>\n",
       "      <td>0.366041</td>\n",
       "      <td>0.493383</td>\n",
       "      <td>0.607406</td>\n",
       "      <td>0.363016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255762</td>\n",
       "      <td>0.147810</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.366295</td>\n",
       "      <td>0.195381</td>\n",
       "      <td>0.127324</td>\n",
       "      <td>0.095386</td>\n",
       "      <td>0.087391</td>\n",
       "      <td>0.096606</td>\n",
       "      <td>0.920487</td>\n",
       "      <td>0.957432</td>\n",
       "      <td>0.965816</td>\n",
       "      <td>0.645856</td>\n",
       "      <td>0.468440</td>\n",
       "      <td>0.396227</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.468440</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.754478</td>\n",
       "      <td>0.597991</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.397314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647341</td>\n",
       "      <td>0.611192</td>\n",
       "      <td>0.597703</td>\n",
       "      <td>0.429781</td>\n",
       "      <td>0.367164</td>\n",
       "      <td>0.497982</td>\n",
       "      <td>0.611768</td>\n",
       "      <td>0.356132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Symbols       Date  bin_gain  ...  Beta_GBPINR  Beta_USDINR  Beta_JPYINR\n",
       "0  20MICRONS.NS 2019-01-01       0.0  ...     0.506552     0.615799     0.372038\n",
       "1  20MICRONS.NS 2019-01-02       0.0  ...     0.506837     0.616610     0.376306\n",
       "2  20MICRONS.NS 2019-01-03       0.0  ...     0.513757     0.612551     0.360319\n",
       "3  20MICRONS.NS 2019-01-04       1.0  ...     0.493383     0.607406     0.363016\n",
       "4  20MICRONS.NS 2019-01-07       0.0  ...     0.497982     0.611768     0.356132\n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols=df_train['Symbols']\n",
    "dates=df_train['Date']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "df_train_scaled_values=scaler.fit_transform(df_train.iloc[:,2:])\n",
    "\n",
    "df_train_scaled=pd.DataFrame(df_train_scaled_values,columns=df_train.columns[2:])\n",
    "df_train_scaled['Symbols']=symbols.values\n",
    "df_train_scaled['Date']=dates.values\n",
    "df_train_scaled=df_train_scaled[df_train.columns]\n",
    "df_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "55akx0Vvnz8P",
    "outputId": "0cfe9027-1054-4270-e6d1-4d783d5e579c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261634, 15, 45)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "y_train_names=[]\n",
    "def prepare_train_arr(df):\n",
    "    for i in range(15,len(df)):\n",
    "        X_train.append(df.iloc[i-15:i,4:].values)\n",
    "        y_train.append(df.iloc[i,4])\n",
    "        y_train_names.append(df.iloc[i,0])\n",
    "    \n",
    "df_train_scaled.groupby([\"Symbols\"]).apply(prepare_train_arr)\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "jsfYNbEsoAvu",
    "outputId": "bdcfdbe5-8d8b-4901-8c48-c24b8075c608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323375, 49)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbols</th>\n",
       "      <th>Date</th>\n",
       "      <th>bin_gain</th>\n",
       "      <th>bin_gain_1%</th>\n",
       "      <th>bin_gain_2%</th>\n",
       "      <th>SMA_PCH5</th>\n",
       "      <th>SMA_PCH15</th>\n",
       "      <th>SMA_PCH50</th>\n",
       "      <th>EMA_PCH5</th>\n",
       "      <th>EMA_PCH15</th>\n",
       "      <th>EMA_PCH50</th>\n",
       "      <th>ROC5</th>\n",
       "      <th>ROC15</th>\n",
       "      <th>ROC50</th>\n",
       "      <th>MOM_PCH5</th>\n",
       "      <th>MOM_PCH15</th>\n",
       "      <th>MOM_PCH50</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>RSI</th>\n",
       "      <th>StochK</th>\n",
       "      <th>StochD</th>\n",
       "      <th>CMO</th>\n",
       "      <th>AroonUp</th>\n",
       "      <th>AroonDown</th>\n",
       "      <th>ChaikinAD</th>\n",
       "      <th>MFI</th>\n",
       "      <th>WillR</th>\n",
       "      <th>ULTOS</th>\n",
       "      <th>MA_BIN5</th>\n",
       "      <th>MA_BIN15</th>\n",
       "      <th>MA_BIN50</th>\n",
       "      <th>EMA_BIN5</th>\n",
       "      <th>EMA_BIN15</th>\n",
       "      <th>EMA_BIN50</th>\n",
       "      <th>ROC_BIN5</th>\n",
       "      <th>ROC_BIN15</th>\n",
       "      <th>ROC_BIN50</th>\n",
       "      <th>MOM_BIN5</th>\n",
       "      <th>MOM_BIN15</th>\n",
       "      <th>MOM_BIN50</th>\n",
       "      <th>macd_BIN</th>\n",
       "      <th>Beta_DOW</th>\n",
       "      <th>Beta_NDQ</th>\n",
       "      <th>Beta_NKK</th>\n",
       "      <th>Beta_NFTY</th>\n",
       "      <th>Beta_EURINR</th>\n",
       "      <th>Beta_GBPINR</th>\n",
       "      <th>Beta_USDINR</th>\n",
       "      <th>Beta_JPYINR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272888</td>\n",
       "      <td>0.153890</td>\n",
       "      <td>0.118366</td>\n",
       "      <td>0.382373</td>\n",
       "      <td>0.204310</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.100874</td>\n",
       "      <td>0.093447</td>\n",
       "      <td>0.092397</td>\n",
       "      <td>0.924910</td>\n",
       "      <td>0.960334</td>\n",
       "      <td>0.964154</td>\n",
       "      <td>0.645913</td>\n",
       "      <td>0.578339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.578339</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.754551</td>\n",
       "      <td>0.559276</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.410535</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659376</td>\n",
       "      <td>0.621633</td>\n",
       "      <td>0.613479</td>\n",
       "      <td>0.440109</td>\n",
       "      <td>0.374381</td>\n",
       "      <td>0.506552</td>\n",
       "      <td>0.615799</td>\n",
       "      <td>0.372038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268255</td>\n",
       "      <td>0.152835</td>\n",
       "      <td>0.117679</td>\n",
       "      <td>0.376302</td>\n",
       "      <td>0.202387</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>0.099852</td>\n",
       "      <td>0.090290</td>\n",
       "      <td>0.094353</td>\n",
       "      <td>0.924120</td>\n",
       "      <td>0.958868</td>\n",
       "      <td>0.964945</td>\n",
       "      <td>0.646127</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.744074</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.754507</td>\n",
       "      <td>0.678977</td>\n",
       "      <td>0.582417</td>\n",
       "      <td>0.382327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659186</td>\n",
       "      <td>0.622859</td>\n",
       "      <td>0.612647</td>\n",
       "      <td>0.438002</td>\n",
       "      <td>0.372540</td>\n",
       "      <td>0.506837</td>\n",
       "      <td>0.616610</td>\n",
       "      <td>0.376306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.150536</td>\n",
       "      <td>0.115880</td>\n",
       "      <td>0.369681</td>\n",
       "      <td>0.198916</td>\n",
       "      <td>0.129764</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.087824</td>\n",
       "      <td>0.093363</td>\n",
       "      <td>0.922739</td>\n",
       "      <td>0.957652</td>\n",
       "      <td>0.964549</td>\n",
       "      <td>0.646143</td>\n",
       "      <td>0.518346</td>\n",
       "      <td>0.698114</td>\n",
       "      <td>0.874214</td>\n",
       "      <td>0.518346</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.639004</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.413571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665346</td>\n",
       "      <td>0.628724</td>\n",
       "      <td>0.615041</td>\n",
       "      <td>0.449382</td>\n",
       "      <td>0.380856</td>\n",
       "      <td>0.513757</td>\n",
       "      <td>0.612551</td>\n",
       "      <td>0.360319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253621</td>\n",
       "      <td>0.146179</td>\n",
       "      <td>0.112324</td>\n",
       "      <td>0.361238</td>\n",
       "      <td>0.193167</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.085590</td>\n",
       "      <td>0.095790</td>\n",
       "      <td>0.919647</td>\n",
       "      <td>0.956493</td>\n",
       "      <td>0.965505</td>\n",
       "      <td>0.645930</td>\n",
       "      <td>0.441681</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.441681</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.754478</td>\n",
       "      <td>0.617409</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.381936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.643412</td>\n",
       "      <td>0.603874</td>\n",
       "      <td>0.597839</td>\n",
       "      <td>0.439958</td>\n",
       "      <td>0.366041</td>\n",
       "      <td>0.493383</td>\n",
       "      <td>0.607406</td>\n",
       "      <td>0.363016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20MICRONS.NS</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255762</td>\n",
       "      <td>0.147810</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.366295</td>\n",
       "      <td>0.195381</td>\n",
       "      <td>0.127324</td>\n",
       "      <td>0.095386</td>\n",
       "      <td>0.087391</td>\n",
       "      <td>0.096606</td>\n",
       "      <td>0.920487</td>\n",
       "      <td>0.957432</td>\n",
       "      <td>0.965816</td>\n",
       "      <td>0.645856</td>\n",
       "      <td>0.468440</td>\n",
       "      <td>0.396227</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.468440</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.754478</td>\n",
       "      <td>0.597991</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.397314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647341</td>\n",
       "      <td>0.611192</td>\n",
       "      <td>0.597703</td>\n",
       "      <td>0.429781</td>\n",
       "      <td>0.367164</td>\n",
       "      <td>0.497982</td>\n",
       "      <td>0.611768</td>\n",
       "      <td>0.356132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Symbols       Date  bin_gain  ...  Beta_GBPINR  Beta_USDINR  Beta_JPYINR\n",
       "0  20MICRONS.NS 2019-01-01       0.0  ...     0.506552     0.615799     0.372038\n",
       "1  20MICRONS.NS 2019-01-02       0.0  ...     0.506837     0.616610     0.376306\n",
       "2  20MICRONS.NS 2019-01-03       0.0  ...     0.513757     0.612551     0.360319\n",
       "3  20MICRONS.NS 2019-01-04       1.0  ...     0.493383     0.607406     0.363016\n",
       "4  20MICRONS.NS 2019-01-07       0.0  ...     0.497982     0.611768     0.356132\n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_symbols=df_2019_2020['Symbols']\n",
    "df_dates=df_2019_2020['Date']\n",
    "df_scaled_values=scaler.transform(df_2019_2020.iloc[:,2:])\n",
    "df_scaled=pd.DataFrame(df_scaled_values,columns=df_2019_2020.columns[2:])\n",
    "df_scaled['Symbols']=df_symbols.values\n",
    "df_scaled['Date']=df_dates.values\n",
    "df_scaled=df_scaled[df_2019_2020.columns]\n",
    "print(df_scaled.shape)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q7w1RfKtttBx",
    "outputId": "9567e0ba-2187-4eb6-c58d-11558d7be6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42001, 15, 45)\n"
     ]
    }
   ],
   "source": [
    "X_test=[]\n",
    "y_test=[]\n",
    "y_test_names=[]\n",
    "def prepare_test(df):\n",
    "  if(not any(df.Date>'2020-01-01')):\n",
    "    return\n",
    "  start_index=np.where(df.Date>='2020-01-01')[0][0]\n",
    "\n",
    "  for i in range(start_index,len(df)):\n",
    "    if(i<15):\n",
    "      continue\n",
    "    X_test.append(df.iloc[i-15:i,4:].values)\n",
    "    y_test.append(df.iloc[i,4])\n",
    "    y_test_names.append(df.iloc[i,0])\n",
    " \n",
    "df_scaled.groupby([\"Symbols\"]).apply(prepare_test)\n",
    "X_test=np.array(X_test)\n",
    "print(X_test.shape)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "7P0_QWIHoYH6",
    "outputId": "f7e59dd6-3177-4b8f-d52b-50f200dc9ad2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    270302\n",
       "1.0     53073\n",
       "Name: bin_gain_2%, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled['bin_gain_2%'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zlHqoJQxx4U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeZhh0e21EKy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6vdyYj_1EbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbBd7_qToaga"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ispU5bHopO5"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(units=50,input_shape=(X_train.shape[1],45)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QIP5cdNio2ld",
    "outputId": "d4526a63-600e-4e1e-e70e-e693692b06d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 175294 samples, validate on 86340 samples\n",
      "Epoch 1/100\n",
      "175294/175294 [==============================] - 14s 80us/step - loss: 0.4460 - binary_accuracy: 0.8344 - val_loss: 0.4435 - val_binary_accuracy: 0.8346\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44350, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4394 - binary_accuracy: 0.8353 - val_loss: 0.4401 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44350 to 0.44008, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4381 - binary_accuracy: 0.8353 - val_loss: 0.4378 - val_binary_accuracy: 0.8346\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44008 to 0.43779, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4375 - binary_accuracy: 0.8353 - val_loss: 0.4370 - val_binary_accuracy: 0.8346\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43779 to 0.43700, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4366 - binary_accuracy: 0.8353 - val_loss: 0.4374 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43700\n",
      "Epoch 6/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4365 - binary_accuracy: 0.8354 - val_loss: 0.4356 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43700 to 0.43556, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4361 - binary_accuracy: 0.8355 - val_loss: 0.4358 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43556\n",
      "Epoch 8/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4358 - binary_accuracy: 0.8356 - val_loss: 0.4356 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43556\n",
      "Epoch 9/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4352 - binary_accuracy: 0.8356 - val_loss: 0.4344 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.43556 to 0.43443, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4351 - binary_accuracy: 0.8356 - val_loss: 0.4350 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43443\n",
      "Epoch 11/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4349 - binary_accuracy: 0.8357 - val_loss: 0.4344 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.43443 to 0.43443, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4345 - binary_accuracy: 0.8358 - val_loss: 0.4347 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43443\n",
      "Epoch 13/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4346 - binary_accuracy: 0.8358 - val_loss: 0.4341 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.43443 to 0.43405, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4344 - binary_accuracy: 0.8356 - val_loss: 0.4350 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43405\n",
      "Epoch 15/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4342 - binary_accuracy: 0.8357 - val_loss: 0.4342 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43405\n",
      "Epoch 16/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4342 - binary_accuracy: 0.8358 - val_loss: 0.4344 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43405\n",
      "Epoch 17/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4338 - binary_accuracy: 0.8357 - val_loss: 0.4339 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.43405 to 0.43390, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4338 - binary_accuracy: 0.8358 - val_loss: 0.4338 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.43390 to 0.43377, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4336 - binary_accuracy: 0.8359 - val_loss: 0.4368 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43377\n",
      "Epoch 20/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4332 - binary_accuracy: 0.8360 - val_loss: 0.4341 - val_binary_accuracy: 0.8354\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43377\n",
      "Epoch 21/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4330 - binary_accuracy: 0.8358 - val_loss: 0.4339 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43377\n",
      "Epoch 22/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4330 - binary_accuracy: 0.8359 - val_loss: 0.4335 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.43377 to 0.43352, saving model to best_model.h5\n",
      "Epoch 23/100\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.4322 - binary_accuracy: 0.8359 - val_loss: 0.4342 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43352\n",
      "Epoch 24/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4325 - binary_accuracy: 0.8359 - val_loss: 0.4339 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43352\n",
      "Epoch 25/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4318 - binary_accuracy: 0.8359 - val_loss: 0.4337 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43352\n",
      "Epoch 26/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4316 - binary_accuracy: 0.8360 - val_loss: 0.4339 - val_binary_accuracy: 0.8354\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43352\n",
      "Epoch 27/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4311 - binary_accuracy: 0.8362 - val_loss: 0.4333 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.43352 to 0.43329, saving model to best_model.h5\n",
      "Epoch 28/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4310 - binary_accuracy: 0.8361 - val_loss: 0.4344 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.43329\n",
      "Epoch 29/100\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.4308 - binary_accuracy: 0.8361 - val_loss: 0.4334 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.43329\n",
      "Epoch 30/100\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.4306 - binary_accuracy: 0.8362 - val_loss: 0.4335 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.43329\n",
      "Epoch 31/100\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.4303 - binary_accuracy: 0.8363 - val_loss: 0.4337 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.43329\n",
      "Epoch 32/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4300 - binary_accuracy: 0.8364 - val_loss: 0.4345 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.43329\n",
      "Epoch 33/100\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.4297 - binary_accuracy: 0.8364 - val_loss: 0.4330 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.43329 to 0.43304, saving model to best_model.h5\n",
      "Epoch 34/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4296 - binary_accuracy: 0.8363 - val_loss: 0.4330 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.43304 to 0.43299, saving model to best_model.h5\n",
      "Epoch 35/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4291 - binary_accuracy: 0.8364 - val_loss: 0.4327 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.43299 to 0.43265, saving model to best_model.h5\n",
      "Epoch 36/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4288 - binary_accuracy: 0.8364 - val_loss: 0.4331 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43265\n",
      "Epoch 37/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4286 - binary_accuracy: 0.8367 - val_loss: 0.4325 - val_binary_accuracy: 0.8354\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.43265 to 0.43248, saving model to best_model.h5\n",
      "Epoch 38/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4283 - binary_accuracy: 0.8365 - val_loss: 0.4332 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43248\n",
      "Epoch 39/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4285 - binary_accuracy: 0.8367 - val_loss: 0.4338 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.43248\n",
      "Epoch 40/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4279 - binary_accuracy: 0.8368 - val_loss: 0.4329 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43248\n",
      "Epoch 41/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4275 - binary_accuracy: 0.8366 - val_loss: 0.4336 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43248\n",
      "Epoch 42/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4274 - binary_accuracy: 0.8369 - val_loss: 0.4347 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.43248\n",
      "Epoch 43/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4269 - binary_accuracy: 0.8369 - val_loss: 0.4342 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43248\n",
      "Epoch 44/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4268 - binary_accuracy: 0.8368 - val_loss: 0.4331 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.43248\n",
      "Epoch 45/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4264 - binary_accuracy: 0.8372 - val_loss: 0.4332 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.43248\n",
      "Epoch 46/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4259 - binary_accuracy: 0.8369 - val_loss: 0.4349 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.43248\n",
      "Epoch 47/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4260 - binary_accuracy: 0.8369 - val_loss: 0.4332 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.43248\n",
      "Epoch 48/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4258 - binary_accuracy: 0.8371 - val_loss: 0.4331 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.43248\n",
      "Epoch 49/100\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.4253 - binary_accuracy: 0.8374 - val_loss: 0.4345 - val_binary_accuracy: 0.8353\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.43248\n",
      "Epoch 50/100\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.4248 - binary_accuracy: 0.8373 - val_loss: 0.4343 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43248\n",
      "Epoch 51/100\n",
      "175294/175294 [==============================] - 11s 64us/step - loss: 0.4247 - binary_accuracy: 0.8375 - val_loss: 0.4334 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.43248\n",
      "Epoch 52/100\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.4243 - binary_accuracy: 0.8378 - val_loss: 0.4342 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43248\n",
      "Epoch 53/100\n",
      "175294/175294 [==============================] - 11s 65us/step - loss: 0.4239 - binary_accuracy: 0.8375 - val_loss: 0.4348 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43248\n",
      "Epoch 54/100\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.4238 - binary_accuracy: 0.8374 - val_loss: 0.4348 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43248\n",
      "Epoch 55/100\n",
      "175294/175294 [==============================] - 11s 65us/step - loss: 0.4232 - binary_accuracy: 0.8377 - val_loss: 0.4346 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.43248\n",
      "Epoch 56/100\n",
      "175294/175294 [==============================] - 11s 64us/step - loss: 0.4230 - binary_accuracy: 0.8377 - val_loss: 0.4347 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.43248\n",
      "Epoch 57/100\n",
      "175294/175294 [==============================] - 11s 64us/step - loss: 0.4226 - binary_accuracy: 0.8378 - val_loss: 0.4342 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.43248\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f52da612518>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=500, nb_epoch=100,validation_split=0.33,callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mY_GSPTbo67h"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RZdDDjU-tSNi",
    "outputId": "ae26380b-2965-4c9c-c8a7-b5f933dd14dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision is  0.6835585585585585\n",
      "Train Accuracy is  0.8362521690605961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,accuracy_score,classification_report\n",
    "y_predicted=saved_model.predict_classes(X_train)\n",
    "y_predicted=y_predicted.reshape(1,-1)[0]\n",
    "precision = precision_score(y_train, y_predicted)\n",
    "accuracy=accuracy_score(y_train,y_predicted)\n",
    "print(\"Train Precision is \",precision)\n",
    "print(\"Train Accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "5m0k2wwiurC_",
    "outputId": "66b0a82d-1272-4439-e054-49f47db3944b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>218185</td>\n",
       "      <td>281</td>\n",
       "      <td>218466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>42561</td>\n",
       "      <td>607</td>\n",
       "      <td>43168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>260746</td>\n",
       "      <td>888</td>\n",
       "      <td>261634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted       0    1     All\n",
       "True                          \n",
       "0.0        218185  281  218466\n",
       "1.0         42561  607   43168\n",
       "All        260746  888  261634"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_train, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kbwotOGAtS-k",
    "outputId": "0598f777-ed25-44da-810d-880802ae2b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision is  0.6634615384615384\n",
      "Test Accuracy is  0.8140520463798481\n"
     ]
    }
   ],
   "source": [
    "y_predicted=saved_model.predict_classes(X_test)\n",
    "y_predicted=y_predicted.reshape(1,-1)[0]\n",
    "precision = precision_score(y_test, y_predicted)\n",
    "accuracy=accuracy_score(y_test,y_predicted)\n",
    "print(\"Test Precision is \",precision)\n",
    "print(\"Test Accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "ybDIEv92tgnx",
    "outputId": "e9ef13c7-46e0-4840-dae3-f66d79d807c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>34122</td>\n",
       "      <td>35</td>\n",
       "      <td>34157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>7775</td>\n",
       "      <td>69</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>41897</td>\n",
       "      <td>104</td>\n",
       "      <td>42001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted      0    1    All\n",
       "True                        \n",
       "0.0        34122   35  34157\n",
       "1.0         7775   69   7844\n",
       "All        41897  104  42001"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCocIW-gSAC1"
   },
   "outputs": [],
   "source": [
    "### Each day of February "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jiR4lA879CI0"
   },
   "outputs": [],
   "source": [
    "X_testdaily=[]\n",
    "y_testdaily=[]\n",
    "ithday_date=[]\n",
    "\n",
    "dates=pd.date_range(start=\"2020-02-01\",end=\"2020-03-01\",freq=\"D\")\n",
    "\n",
    "def prepare_test_daily(df,i):\n",
    "  global X_test_ithday\n",
    "  global y_test_ithday\n",
    "\n",
    "  if(not any(df.Date == i)):\n",
    "    return\n",
    "  start_index=np.where(df.Date==i)[0][0]\n",
    "\n",
    "  if(start_index<15):\n",
    "      return\n",
    "      \n",
    "  X_test_ithday.append(df.iloc[start_index-15:start_index,4:].values)\n",
    "  \n",
    "  \n",
    "\n",
    "  y_test_ithday.append(df.iloc[start_index,4])\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "for i in dates:\n",
    "  X_test_ithday=list()\n",
    "  y_test_ithday=list()\n",
    "  \n",
    "  df_scaled.groupby([\"Symbols\"]).apply(prepare_test_daily, i)\n",
    "\n",
    "  X_test_ithday=np.array(X_test_ithday)\n",
    "  y_test_ithday=np.array(y_test_ithday)\n",
    "\n",
    "  if(X_test_ithday.shape == (0,)):\n",
    "    continue\n",
    "\n",
    "  X_testdaily.append(X_test_ithday)\n",
    "  y_testdaily.append(y_test_ithday)\n",
    "  ithday_date.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FWgCISpQCwFV",
    "outputId": "b1c64c9e-2539-4ea1-b6b0-fb32ed827f0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_testdaily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "Ex_C79oa-7Vw",
    "outputId": "a95246cb-b9be-4c5c-f1f4-3666f4491fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Per Day Accuracy for the month of February 2020 -  0.808915654019345\n",
      "Standard Deviation of accuracies for the month of February 2020 - 0.09336831793637589\n",
      "\n",
      "Average Precision per Day for the month of February 2020 -  0.814102564102564\n",
      "Standard Deviations of precisions for the month of February 2020 - 0.2492592576310715\n"
     ]
    }
   ],
   "source": [
    "precisions=[]\n",
    "accuracies=[]\n",
    "for i in range(13):\n",
    "  ithday_predictions=saved_model.predict_classes(X_testdaily[i])\n",
    "  ithday_predictions=ithday_predictions.reshape(1,-1)[0]\n",
    "  precision = precision_score(y_testdaily[i],ithday_predictions)\n",
    "  # print(precision , i)\n",
    "  accuracy=accuracy_score(y_testdaily[i],ithday_predictions)\n",
    "  precisions.append(precision)\n",
    "  accuracies.append(accuracy)\n",
    "accuracies=np.array(accuracies)\n",
    "precisions=np.array(precisions)\n",
    "\n",
    "print(\"Average Per Day Accuracy for the month of February 2020 - \",accuracies.mean())\n",
    "print(\"Standard Deviation of accuracies for the month of February 2020 -\",accuracies.std())\n",
    "print(\"\\nAverage Precision per Day for the month of February 2020 - \",precisions.mean())\n",
    "print(\"Standard Deviations of precisions for the month of February 2020 -\",precisions.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zji65YYSE2M"
   },
   "outputs": [],
   "source": [
    "## Each day of January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qk8xphM99MIn"
   },
   "outputs": [],
   "source": [
    "X_testdaily=[]\n",
    "y_testdaily=[]\n",
    "ithday_date=[]\n",
    "\n",
    "dates=pd.date_range(start=\"2020-01-01\",end=\"2020-02-01\",freq=\"D\")\n",
    "\n",
    "def prepare_test_daily(df,i):\n",
    "  global X_test_ithday\n",
    "  global y_test_ithday\n",
    "\n",
    "  if(not any(df.Date == i)):\n",
    "    return\n",
    "  start_index=np.where(df.Date==i)[0][0]\n",
    "\n",
    "  if(start_index<15):\n",
    "      return\n",
    "      \n",
    "  X_test_ithday.append(df.iloc[start_index-15:start_index,4:].values)\n",
    "  \n",
    "  \n",
    "\n",
    "  y_test_ithday.append(df.iloc[start_index,4])\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "for i in dates:\n",
    "  X_test_ithday=list()\n",
    "  y_test_ithday=list()\n",
    "  \n",
    "  df_scaled.groupby([\"Symbols\"]).apply(prepare_test_daily, i)\n",
    "\n",
    "  X_test_ithday=np.array(X_test_ithday)\n",
    "  y_test_ithday=np.array(y_test_ithday)\n",
    "\n",
    "  if(X_test_ithday.shape == (0,)):\n",
    "    continue\n",
    "\n",
    "  X_testdaily.append(X_test_ithday)\n",
    "  y_testdaily.append(y_test_ithday)\n",
    "  ithday_date.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wzX6KJr8IYYJ",
    "outputId": "23e1c614-d360-4f6c-ddda-129a04108173"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_testdaily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "lO9vZyvhSLNW",
    "outputId": "2240be13-ee43-44dd-fa12-b797938ec70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Per Day Accuracy for the month of January 2020 -  0.7722399594903349\n",
      "Standard Deviation of accuracies for the month of January 2020 - 0.11226027329855881\n",
      "\n",
      "Average Precision per Day for the month of January 2020 -  0.5911671661671661\n",
      "Standard Deviations of precisions for the month of January 2020 - 0.2697101902961413\n"
     ]
    }
   ],
   "source": [
    "precisions=[]\n",
    "accuracies=[]\n",
    "for i in range(13):\n",
    "  ithday_predictions=saved_model.predict_classes(X_testdaily[i])\n",
    "  ithday_predictions=ithday_predictions.reshape(1,-1)[0]\n",
    "  precision = precision_score(y_testdaily[i],ithday_predictions)\n",
    "  # print(precision , i)\n",
    "  accuracy=accuracy_score(y_testdaily[i],ithday_predictions)\n",
    "  precisions.append(precision)\n",
    "  accuracies.append(accuracy)\n",
    "accuracies=np.array(accuracies)\n",
    "precisions=np.array(precisions)\n",
    "\n",
    "print(\"Average Per Day Accuracy for the month of January 2020 - \",accuracies.mean())\n",
    "print(\"Standard Deviation of accuracies for the month of January 2020 -\",accuracies.std())\n",
    "print(\"\\nAverage Precision per Day for the month of January 2020 - \",precisions.mean())\n",
    "print(\"Standard Deviations of precisions for the month of January 2020 -\",precisions.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "MwuO7eKb5jFr",
    "outputId": "3eb3cb4d-51f1-4e13-dda7-9a78232ce0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision is  0.75\n",
      "Train Accuracy is  0.9637618636755824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1111</td>\n",
       "      <td>2</td>\n",
       "      <td>1113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1151</td>\n",
       "      <td>8</td>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0  1   All\n",
       "True                    \n",
       "0.0        1111  2  1113\n",
       "1.0          40  6    46\n",
       "All        1151  8  1159"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jan_2nd_predictions = saved_model.predict_classes(X_test_2ndjan)\n",
    "jan_2nd_predictions=jan_2nd_predictions.reshape(1,-1)[0]\n",
    "\n",
    "precision = precision_score(y_test_2ndjan, jan_2nd_predictions)\n",
    "accuracy=accuracy_score(y_test_2ndjan,jan_2nd_predictions)\n",
    "\n",
    "print(\"Train Precision is \",precision)\n",
    "print(\"Train Accuracy is \",accuracy)\n",
    "\n",
    "pd.crosstab(y_test_2ndjan, jan_2nd_predictions, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_ofu5bTSPTs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hxeTzSz5rYF"
   },
   "outputs": [],
   "source": [
    "### IGNore code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yK3d8LA5rt4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiiiV0Km849g"
   },
   "outputs": [],
   "source": [
    "### Ignore COde below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVH8rmJB85OQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xper5HO485by"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyRbh_VhuhXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jlk5Ks2vW1A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKdyxZ8fwZPG"
   },
   "outputs": [],
   "source": [
    "### This is another model I tried but is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKq-a-1hwfAA"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2SL_SBgfzmmX",
    "outputId": "fc1f3014-4166-49a0-ac74-07e54053281c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision is  0.68\n",
      "Train Accuracy is  0.8362445247941781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,accuracy_score,classification_report\n",
    "y_predicted=saved_model.predict_classes(X_train)\n",
    "y_predicted=y_predicted.reshape(1,-1)[0]\n",
    "precision = precision_score(y_train, y_predicted)\n",
    "accuracy=accuracy_score(y_train,y_predicted)\n",
    "print(\"Train Precision is \",precision)\n",
    "print(\"Train Accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "4-FPuEEDz8g7",
    "outputId": "8b15b1ad-a7ec-4ab9-a814-92ce1ae91edb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>218178</td>\n",
       "      <td>288</td>\n",
       "      <td>218466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>42556</td>\n",
       "      <td>612</td>\n",
       "      <td>43168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>260734</td>\n",
       "      <td>900</td>\n",
       "      <td>261634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted       0    1     All\n",
       "True                          \n",
       "0.0        218178  288  218466\n",
       "1.0         42556  612   43168\n",
       "All        260734  900  261634"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_train, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wFdaQ-mEzqDc",
    "outputId": "285d9453-1293-46f3-c1fc-e54611932da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision is  0.8181818181818182\n",
      "Test Accuracy is  0.8140758553367777\n"
     ]
    }
   ],
   "source": [
    "y_predicted=saved_model.predict_classes(X_test)\n",
    "y_predicted=y_predicted.reshape(1,-1)[0]\n",
    "precision = precision_score(y_test, y_predicted)\n",
    "accuracy=accuracy_score(y_test,y_predicted)\n",
    "print(\"Test Precision is \",precision)\n",
    "print(\"Test Accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "-iwBSklVzyxX",
    "outputId": "2aabddd0-9f85-4545-94a7-593a7e32f1e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>34147</td>\n",
       "      <td>10</td>\n",
       "      <td>34157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>7799</td>\n",
       "      <td>45</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>41946</td>\n",
       "      <td>55</td>\n",
       "      <td>42001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted      0   1    All\n",
       "True                       \n",
       "0.0        34147  10  34157\n",
       "1.0         7799  45   7844\n",
       "All        41946  55  42001"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cayaspWz4sH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gR6oteU4elV"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(units=50,input_shape=(X_train.shape[1],45)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "class_weight={0:0.40,\n",
    "               1:0.60}\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R2OCcuSg4vZ2",
    "outputId": "d6d97314-2292-4af8-ad69-d0ae89cae7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 175294 samples, validate on 86340 samples\n",
      "Epoch 1/200\n",
      "\r",
      "   500/175294 [..............................] - ETA: 17s - loss: 0.2499 - binary_accuracy: 0.8120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2291 - binary_accuracy: 0.8352 - val_loss: 0.2275 - val_binary_accuracy: 0.8344\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22755, saving model to best_model.h5\n",
      "Epoch 2/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2277 - binary_accuracy: 0.8349 - val_loss: 0.2272 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22755 to 0.22719, saving model to best_model.h5\n",
      "Epoch 3/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2271 - binary_accuracy: 0.8342 - val_loss: 0.2262 - val_binary_accuracy: 0.8348\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22719 to 0.22621, saving model to best_model.h5\n",
      "Epoch 4/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2269 - binary_accuracy: 0.8348 - val_loss: 0.2262 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22621 to 0.22617, saving model to best_model.h5\n",
      "Epoch 5/200\n",
      "175294/175294 [==============================] - 11s 65us/step - loss: 0.2266 - binary_accuracy: 0.8347 - val_loss: 0.2265 - val_binary_accuracy: 0.8347\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22617\n",
      "Epoch 6/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2264 - binary_accuracy: 0.8346 - val_loss: 0.2259 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22617 to 0.22589, saving model to best_model.h5\n",
      "Epoch 7/200\n",
      "175294/175294 [==============================] - 11s 64us/step - loss: 0.2261 - binary_accuracy: 0.8349 - val_loss: 0.2261 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22589\n",
      "Epoch 8/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2260 - binary_accuracy: 0.8346 - val_loss: 0.2271 - val_binary_accuracy: 0.8273\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22589\n",
      "Epoch 9/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2260 - binary_accuracy: 0.8348 - val_loss: 0.2262 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22589\n",
      "Epoch 10/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2257 - binary_accuracy: 0.8348 - val_loss: 0.2257 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22589 to 0.22567, saving model to best_model.h5\n",
      "Epoch 11/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2256 - binary_accuracy: 0.8347 - val_loss: 0.2258 - val_binary_accuracy: 0.8315\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22567\n",
      "Epoch 12/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2255 - binary_accuracy: 0.8347 - val_loss: 0.2253 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.22567 to 0.22530, saving model to best_model.h5\n",
      "Epoch 13/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2254 - binary_accuracy: 0.8347 - val_loss: 0.2252 - val_binary_accuracy: 0.8333\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22530 to 0.22518, saving model to best_model.h5\n",
      "Epoch 14/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2254 - binary_accuracy: 0.8347 - val_loss: 0.2278 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22518\n",
      "Epoch 15/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2251 - binary_accuracy: 0.8349 - val_loss: 0.2251 - val_binary_accuracy: 0.8352\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.22518 to 0.22511, saving model to best_model.h5\n",
      "Epoch 16/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2250 - binary_accuracy: 0.8350 - val_loss: 0.2253 - val_binary_accuracy: 0.8335\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22511\n",
      "Epoch 17/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2249 - binary_accuracy: 0.8352 - val_loss: 0.2255 - val_binary_accuracy: 0.8339\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.22511\n",
      "Epoch 18/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2248 - binary_accuracy: 0.8348 - val_loss: 0.2258 - val_binary_accuracy: 0.8309\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22511\n",
      "Epoch 19/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2247 - binary_accuracy: 0.8347 - val_loss: 0.2257 - val_binary_accuracy: 0.8346\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22511\n",
      "Epoch 20/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2245 - binary_accuracy: 0.8348 - val_loss: 0.2257 - val_binary_accuracy: 0.8339\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22511\n",
      "Epoch 21/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2245 - binary_accuracy: 0.8350 - val_loss: 0.2248 - val_binary_accuracy: 0.8349\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.22511 to 0.22484, saving model to best_model.h5\n",
      "Epoch 22/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2243 - binary_accuracy: 0.8352 - val_loss: 0.2249 - val_binary_accuracy: 0.8345\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22484\n",
      "Epoch 23/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2240 - binary_accuracy: 0.8348 - val_loss: 0.2248 - val_binary_accuracy: 0.8350\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.22484 to 0.22476, saving model to best_model.h5\n",
      "Epoch 24/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2239 - binary_accuracy: 0.8350 - val_loss: 0.2258 - val_binary_accuracy: 0.8347\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.22476\n",
      "Epoch 25/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2236 - binary_accuracy: 0.8349 - val_loss: 0.2250 - val_binary_accuracy: 0.8344\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.22476\n",
      "Epoch 26/200\n",
      "175294/175294 [==============================] - 11s 66us/step - loss: 0.2236 - binary_accuracy: 0.8350 - val_loss: 0.2247 - val_binary_accuracy: 0.8342\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.22476 to 0.22468, saving model to best_model.h5\n",
      "Epoch 27/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2236 - binary_accuracy: 0.8352 - val_loss: 0.2250 - val_binary_accuracy: 0.8345\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.22468\n",
      "Epoch 28/200\n",
      "175294/175294 [==============================] - 11s 65us/step - loss: 0.2234 - binary_accuracy: 0.8350 - val_loss: 0.2246 - val_binary_accuracy: 0.8346\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.22468 to 0.22455, saving model to best_model.h5\n",
      "Epoch 29/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2232 - binary_accuracy: 0.8355 - val_loss: 0.2245 - val_binary_accuracy: 0.8345\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.22455 to 0.22449, saving model to best_model.h5\n",
      "Epoch 30/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2233 - binary_accuracy: 0.8350 - val_loss: 0.2246 - val_binary_accuracy: 0.8347\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.22449\n",
      "Epoch 31/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2228 - binary_accuracy: 0.8352 - val_loss: 0.2250 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.22449\n",
      "Epoch 32/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2229 - binary_accuracy: 0.8349 - val_loss: 0.2246 - val_binary_accuracy: 0.8347\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.22449\n",
      "Epoch 33/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2228 - binary_accuracy: 0.8354 - val_loss: 0.2246 - val_binary_accuracy: 0.8341\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.22449\n",
      "Epoch 34/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2226 - binary_accuracy: 0.8351 - val_loss: 0.2247 - val_binary_accuracy: 0.8351\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.22449\n",
      "Epoch 35/200\n",
      "175294/175294 [==============================] - 13s 71us/step - loss: 0.2225 - binary_accuracy: 0.8354 - val_loss: 0.2247 - val_binary_accuracy: 0.8336\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.22449\n",
      "Epoch 36/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2224 - binary_accuracy: 0.8351 - val_loss: 0.2249 - val_binary_accuracy: 0.8340\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.22449\n",
      "Epoch 37/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2222 - binary_accuracy: 0.8353 - val_loss: 0.2251 - val_binary_accuracy: 0.8343\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.22449\n",
      "Epoch 38/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2220 - binary_accuracy: 0.8356 - val_loss: 0.2246 - val_binary_accuracy: 0.8342\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.22449\n",
      "Epoch 39/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2219 - binary_accuracy: 0.8352 - val_loss: 0.2260 - val_binary_accuracy: 0.8341\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.22449\n",
      "Epoch 40/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2219 - binary_accuracy: 0.8355 - val_loss: 0.2251 - val_binary_accuracy: 0.8328\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22449\n",
      "Epoch 41/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2216 - binary_accuracy: 0.8355 - val_loss: 0.2251 - val_binary_accuracy: 0.8330\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22449\n",
      "Epoch 42/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2213 - binary_accuracy: 0.8356 - val_loss: 0.2250 - val_binary_accuracy: 0.8344\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22449\n",
      "Epoch 43/200\n",
      "175294/175294 [==============================] - 13s 71us/step - loss: 0.2211 - binary_accuracy: 0.8358 - val_loss: 0.2248 - val_binary_accuracy: 0.8347\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22449\n",
      "Epoch 44/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2210 - binary_accuracy: 0.8355 - val_loss: 0.2250 - val_binary_accuracy: 0.8335\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22449\n",
      "Epoch 45/200\n",
      "175294/175294 [==============================] - 13s 71us/step - loss: 0.2207 - binary_accuracy: 0.8360 - val_loss: 0.2251 - val_binary_accuracy: 0.8319\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22449\n",
      "Epoch 46/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2208 - binary_accuracy: 0.8358 - val_loss: 0.2253 - val_binary_accuracy: 0.8332\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.22449\n",
      "Epoch 47/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2206 - binary_accuracy: 0.8360 - val_loss: 0.2252 - val_binary_accuracy: 0.8341\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.22449\n",
      "Epoch 48/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2203 - binary_accuracy: 0.8357 - val_loss: 0.2252 - val_binary_accuracy: 0.8346\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.22449\n",
      "Epoch 49/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2202 - binary_accuracy: 0.8358 - val_loss: 0.2255 - val_binary_accuracy: 0.8329\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.22449\n",
      "Epoch 50/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2200 - binary_accuracy: 0.8360 - val_loss: 0.2254 - val_binary_accuracy: 0.8343\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22449\n",
      "Epoch 51/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2198 - binary_accuracy: 0.8358 - val_loss: 0.2257 - val_binary_accuracy: 0.8313\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.22449\n",
      "Epoch 52/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2197 - binary_accuracy: 0.8358 - val_loss: 0.2250 - val_binary_accuracy: 0.8336\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.22449\n",
      "Epoch 53/200\n",
      "175294/175294 [==============================] - 12s 67us/step - loss: 0.2196 - binary_accuracy: 0.8362 - val_loss: 0.2261 - val_binary_accuracy: 0.8314\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.22449\n",
      "Epoch 54/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2194 - binary_accuracy: 0.8358 - val_loss: 0.2252 - val_binary_accuracy: 0.8341\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.22449\n",
      "Epoch 55/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2191 - binary_accuracy: 0.8363 - val_loss: 0.2256 - val_binary_accuracy: 0.8337\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.22449\n",
      "Epoch 56/200\n",
      "175294/175294 [==============================] - 12s 66us/step - loss: 0.2189 - binary_accuracy: 0.8361 - val_loss: 0.2260 - val_binary_accuracy: 0.8301\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.22449\n",
      "Epoch 57/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2188 - binary_accuracy: 0.8363 - val_loss: 0.2260 - val_binary_accuracy: 0.8317\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.22449\n",
      "Epoch 58/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2185 - binary_accuracy: 0.8366 - val_loss: 0.2260 - val_binary_accuracy: 0.8331\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.22449\n",
      "Epoch 59/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2182 - binary_accuracy: 0.8362 - val_loss: 0.2270 - val_binary_accuracy: 0.8332\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.22449\n",
      "Epoch 60/200\n",
      "175294/175294 [==============================] - 12s 68us/step - loss: 0.2183 - binary_accuracy: 0.8361 - val_loss: 0.2262 - val_binary_accuracy: 0.8327\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.22449\n",
      "Epoch 61/200\n",
      "175294/175294 [==============================] - 13s 74us/step - loss: 0.2177 - binary_accuracy: 0.8366 - val_loss: 0.2258 - val_binary_accuracy: 0.8328\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.22449\n",
      "Epoch 62/200\n",
      "175294/175294 [==============================] - 13s 72us/step - loss: 0.2175 - binary_accuracy: 0.8366 - val_loss: 0.2264 - val_binary_accuracy: 0.8334\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.22449\n",
      "Epoch 63/200\n",
      "175294/175294 [==============================] - 13s 74us/step - loss: 0.2174 - binary_accuracy: 0.8365 - val_loss: 0.2259 - val_binary_accuracy: 0.8328\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.22449\n",
      "Epoch 64/200\n",
      "175294/175294 [==============================] - 12s 71us/step - loss: 0.2171 - binary_accuracy: 0.8362 - val_loss: 0.2270 - val_binary_accuracy: 0.8323\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.22449\n",
      "Epoch 65/200\n",
      "175294/175294 [==============================] - 13s 73us/step - loss: 0.2170 - binary_accuracy: 0.8369 - val_loss: 0.2265 - val_binary_accuracy: 0.8317\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.22449\n",
      "Epoch 66/200\n",
      "175294/175294 [==============================] - 12s 71us/step - loss: 0.2168 - binary_accuracy: 0.8368 - val_loss: 0.2265 - val_binary_accuracy: 0.8318\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.22449\n",
      "Epoch 67/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2164 - binary_accuracy: 0.8368 - val_loss: 0.2274 - val_binary_accuracy: 0.8321\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.22449\n",
      "Epoch 68/200\n",
      "175294/175294 [==============================] - 12s 70us/step - loss: 0.2163 - binary_accuracy: 0.8365 - val_loss: 0.2273 - val_binary_accuracy: 0.8324\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.22449\n",
      "Epoch 69/200\n",
      "175294/175294 [==============================] - 13s 72us/step - loss: 0.2162 - binary_accuracy: 0.8367 - val_loss: 0.2279 - val_binary_accuracy: 0.8326\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.22449\n",
      "Epoch 70/200\n",
      "175294/175294 [==============================] - 13s 74us/step - loss: 0.2157 - binary_accuracy: 0.8373 - val_loss: 0.2278 - val_binary_accuracy: 0.8293\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.22449\n",
      "Epoch 71/200\n",
      "175294/175294 [==============================] - 13s 73us/step - loss: 0.2154 - binary_accuracy: 0.8371 - val_loss: 0.2271 - val_binary_accuracy: 0.8336\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.22449\n",
      "Epoch 72/200\n",
      "175294/175294 [==============================] - 13s 72us/step - loss: 0.2153 - binary_accuracy: 0.8373 - val_loss: 0.2278 - val_binary_accuracy: 0.8327\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.22449\n",
      "Epoch 73/200\n",
      "175294/175294 [==============================] - 12s 71us/step - loss: 0.2148 - binary_accuracy: 0.8366 - val_loss: 0.2281 - val_binary_accuracy: 0.8328\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.22449\n",
      "Epoch 74/200\n",
      "175294/175294 [==============================] - 12s 69us/step - loss: 0.2145 - binary_accuracy: 0.8369 - val_loss: 0.2278 - val_binary_accuracy: 0.8313\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.22449\n",
      "Epoch 75/200\n",
      "175294/175294 [==============================] - 13s 72us/step - loss: 0.2146 - binary_accuracy: 0.8370 - val_loss: 0.2277 - val_binary_accuracy: 0.8322\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.22449\n",
      "Epoch 76/200\n",
      "175294/175294 [==============================] - 13s 73us/step - loss: 0.2142 - binary_accuracy: 0.8375 - val_loss: 0.2297 - val_binary_accuracy: 0.8317\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.22449\n",
      "Epoch 77/200\n",
      "175294/175294 [==============================] - 13s 73us/step - loss: 0.2139 - binary_accuracy: 0.8370 - val_loss: 0.2291 - val_binary_accuracy: 0.8319\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.22449\n",
      "Epoch 78/200\n",
      "175294/175294 [==============================] - 13s 72us/step - loss: 0.2138 - binary_accuracy: 0.8372 - val_loss: 0.2282 - val_binary_accuracy: 0.8337\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.22449\n",
      "Epoch 79/200\n",
      "175294/175294 [==============================] - 14s 78us/step - loss: 0.2135 - binary_accuracy: 0.8373 - val_loss: 0.2291 - val_binary_accuracy: 0.8329\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.22449\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f650c174860>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=500, nb_epoch=200,validation_split=0.33,callbacks=[es,mc],class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCb_xBD_86Rp"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VtZNFYvg46J2",
    "outputId": "7e87c6da-72b0-481e-91fa-2f2ce8b2543d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision is  0.5363372093023255\n",
      "Train Accuracy is  0.8357705802762638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,accuracy_score,classification_report\n",
    "y_predicted=saved_model.predict_classes(X_train)\n",
    "y_predicted=y_predicted.reshape(1,-1)[0]\n",
    "precision = precision_score(y_train, y_predicted)\n",
    "accuracy=accuracy_score(y_train,y_predicted)\n",
    "print(\"Train Precision is \",precision)\n",
    "print(\"Train Accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "ZL8vPNEN8xXG",
    "outputId": "6c8d0c27-7729-42ee-ced2-62e188aadb65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>217190</td>\n",
       "      <td>1276</td>\n",
       "      <td>218466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>41692</td>\n",
       "      <td>1476</td>\n",
       "      <td>43168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>258882</td>\n",
       "      <td>2752</td>\n",
       "      <td>261634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted       0     1     All\n",
       "True                           \n",
       "0.0        217190  1276  218466\n",
       "1.0         41692  1476   43168\n",
       "All        258882  2752  261634"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_train, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uvTuPiCm9FXE",
    "outputId": "b14443a4-24d7-4c19-98ba-4414dd70b67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision is  0.5664739884393064\n",
      "Test Accuracy is  0.8137901478536226\n"
     ]
    }
   ],
   "source": [
    "y_predicted=saved_model.predict_classes(X_test)\n",
    "y_predicted=y_predicted.reshape(1,-1)[0]\n",
    "precision = precision_score(y_test, y_predicted)\n",
    "accuracy=accuracy_score(y_test,y_predicted)\n",
    "print(\"Test Precision is \",precision)\n",
    "print(\"Test Accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "_AsyvYfL9jkJ",
    "outputId": "0284caee-5638-4ff0-d9e2-fcb2227e2ac9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>34082</td>\n",
       "      <td>75</td>\n",
       "      <td>34157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>7746</td>\n",
       "      <td>98</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>41828</td>\n",
       "      <td>173</td>\n",
       "      <td>42001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted      0    1    All\n",
       "True                        \n",
       "0.0        34082   75  34157\n",
       "1.0         7746   98   7844\n",
       "All        41828  173  42001"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8--2PUNj9myB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Stock Market-8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
